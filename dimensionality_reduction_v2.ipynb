{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dimensionality reduction_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rK9kW_7xHORf",
        "IA4scmitmWQR",
        "_ho03JK3H77y",
        "xnsHi8HTIJqH",
        "pn96G8iiIOaC",
        "A1yWUqTdIQdC",
        "B5b2diYM9Awm",
        "m11RwsPq4M89",
        "l2Zl1UhUeHxJ",
        "i789vF8xHWBL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mimikuo365/COVID19-Infection-and-Mortality-Prediction/blob/main/dimensionality_reduction_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK9kW_7xHORf"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i7zUCFHbp80",
        "outputId": "86e2ac46-165c-4fbd-c496-151158569a3f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA4scmitmWQR"
      },
      "source": [
        "# Run with GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-eYkjhi5Ykm"
      },
      "source": [
        "- [Scikit-learn Tutorial – Beginner’s Guide to GPU Accelerating ML Pipelines\n",
        "](https://developer.nvidia.com/blog/scikit-learn-tutorial-beginners-guide-to-gpu-accelerating-ml-pipelines/)\n",
        "\n",
        "- [Code](https://colab.research.google.com/drive/1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0#forceEdit=true&sandboxMode=true&scrollTo=dCE8WhO3HpL_)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VivvQk53mJbp",
        "outputId": "a66ceb0b-72e2-4f52-cdda-7cb59c03bfe9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov  8 05:27:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    28W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PifMUeMZmVba",
        "outputId": "f821f31f-97ed-4010-9260-7cb3713b641e"
      },
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/env-check.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'rapidsai-csp-utils' already exists and is not an empty directory.\n",
            "Traceback (most recent call last):\n",
            "  File \"rapidsai-csp-utils/colab/env-check.py\", line 24, in <module>\n",
            "    Please use 'Runtime -> Factory Reset Runtimes...', which will allocate you a different GPU instance, to try again.\"\"\"\n",
            "Exception: \n",
            "                  Unfortunately Colab didn't give you a RAPIDS compatible GPU (P4, P100, T4, or V100), but gave you a Tesla K80.\n",
            "\n",
            "                  Please use 'Runtime -> Factory Reset Runtimes...', which will allocate you a different GPU instance, to try again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt7Z86EEmYhs",
        "outputId": "75071997-77ea-4feb-de12-e4727373b5b5"
      },
      "source": [
        "# This will update the Colab environment and restart the kernel.  Don't run the next cell until you see the session crash.\n",
        "!bash rapidsai-csp-utils/colab/update_gcc.sh\n",
        "import os\n",
        "os._exit(00)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating your Colab environment.  This will restart your kernel.  Don't Panic!\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:16 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 1s (192 kB/s)\n",
            "Reading package lists... Done\n",
            "Added repo\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "0% [8 InRelease gpgv 88.7 kB]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH7pZlqwmdkv"
      },
      "source": [
        "# This will install CondaColab.  This will restart your kernel one last time.  Run this cell by itself and only run the next cell once you see the session crash.\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKst8H2JmrjY"
      },
      "source": [
        "# you can now run the rest of the cells as normal\n",
        "import condacolab\n",
        "condacolab.check()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP40lQSJms5U"
      },
      "source": [
        "# Installing RAPIDS is now 'python rapidsai-csp-utils/colab/install_rapids.py <release> <packages>'\n",
        "# The <release> options are 'stable' and 'nightly'.  Leaving it blank or adding any other words will default to stable.\n",
        "# The <packages> option are default blank or 'core'.  By default, we install RAPIDSAI and BlazingSQL.  The 'core' option will install only RAPIDSAI and not include BlazingSQL, \n",
        "!python rapidsai-csp-utils/colab/install_rapids.py stable\n",
        "import os\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "os.environ['CONDA_PREFIX'] = '/usr/local'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6TQXMNKmwCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab1aeb0-cb77-4907-a420-4d23dee21c59"
      },
      "source": [
        "import cuml as sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/site-packages/cudf/utils/gpu_utils.py:93: UserWarning: You will need a GPU with NVIDIA Pascal™ or newer architecture\n",
            "Detected GPU 0: Tesla K80 \n",
            "Detected Compute Capability: 3.7\n",
            "  f\"You will need a GPU with NVIDIA Pascal™ or \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpVHuEf5hUTU"
      },
      "source": [
        "# Setup Environment and Load Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Z_-joFH5T3"
      },
      "source": [
        "## Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPSXSZi1E0Lm",
        "outputId": "8a952ca3-c6dc-41fa-a2d4-5e9bbb6d5078"
      },
      "source": [
        "!pip install scikit-learn==0.24"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.24\n",
            "  Downloading scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.0 threadpoolctl-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf6xQA_1nLAv"
      },
      "source": [
        "# sklearn library\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn import linear_model  \n",
        "from sklearn.svm import SVR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTsy3_Iq21K-"
      },
      "source": [
        "from random import randint, randrange\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlpIqYTQDeKu",
        "outputId": "0359ab38-7cfa-4cef-d5aa-cd460f9000fb"
      },
      "source": [
        "!pip install epiweeks\n",
        "from epiweeks import Week, Year\n",
        "from datetime import date"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting epiweeks\n",
            "  Downloading epiweeks-2.1.3-py3-none-any.whl (5.9 kB)\n",
            "Installing collected packages: epiweeks\n",
            "Successfully installed epiweeks-2.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ho03JK3H77y"
      },
      "source": [
        "## Pickle Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8GyU25P7XXj"
      },
      "source": [
        "def loadPickle(path):\n",
        "  file = open(path, \"rb\")\n",
        "  output = pickle.load(file)\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtcpGXzi9_KR"
      },
      "source": [
        "def saveAsPickle(path, dictionary_data):\n",
        "  file = open(path, \"wb\")\n",
        "  pickle.dump(dictionary_data, file)\n",
        "  file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLtYlWDcxW2_"
      },
      "source": [
        "def loadDataFromPickle(path):\n",
        "  # Load pickle file from folder\n",
        "  info_dict = loadPickle(path)\n",
        "  return info_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnsHi8HTIJqH"
      },
      "source": [
        "## Math Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn-z5IIbxNFq"
      },
      "source": [
        "def getEpiWeek(origin_str):\n",
        "  date_ls = origin_str.split('-')\n",
        "  return Week.fromdate(date(int(date_ls[0]), int(date_ls[1]), int(date_ls[2])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn96G8iiIOaC"
      },
      "source": [
        "## Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1Q7rP1Jdn5S"
      },
      "source": [
        "def loadStructuredData(csv_path):\n",
        "  df = pd.DataFrame()\n",
        "  if os.path.isdir(csv_path):\n",
        "    for filename in os.listdir(csv_path):\n",
        "      file_path = os.path.join(csv_path, filename)\n",
        "      df = df.append(pd.read_csv(file_path))\n",
        "  elif os.path.isfile(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "  else:\n",
        "    print('Error: Not folder or file')\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1yWUqTdIQdC"
      },
      "source": [
        "## Image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jveGkUZr99De"
      },
      "source": [
        "def plotData(x_asix, y_axis, title):\n",
        "  plt.figure(figsize = (12, 8))\n",
        "  plt.plot(x_axis, y_axis, color ='black')\n",
        "  plt.ylabel('Dengue Cases')\n",
        "  plt.title(title)\n",
        "  plt.tick_params(axis='both',labelsize=3)\n",
        "  plt.setp(plt.gca().get_xticklabels(), rotation=30, horizontalalignment='right')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvyE1Bzo7HKL"
      },
      "source": [
        "def readImg(img_path, resize_ratio=None):\n",
        "  img = io.imread(img_path)\n",
        "  print(os.path.basename(img_path))\n",
        "  print('    origin shape:'.ljust(10), img.shape)\n",
        "\n",
        "  if resize_ratio:\n",
        "    new_shape = (x / resize_ratio for x in img.shape)\n",
        "    img = resize(img, resize_ratio)\n",
        "  print('    resized shape:'.ljust(10), img.shape)\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghwVM6HzIeOp"
      },
      "source": [
        "## Obtain both structural data and images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPhprhk73Yj1"
      },
      "source": [
        "def combineData(img_folder, df, resize_ratio=None):\n",
        "  info_dict = {'LastDayWeek':[], 'cases_medellin':[], 'Image':[]}\n",
        "  img_list = os.listdir(img_folder)\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    name = row['LastDayWeek']\n",
        "    week_df = str(getEpiWeek(name))\n",
        "    case = row['cases_medellin']\n",
        "    print(name)\n",
        "    for img_name in img_list:\n",
        "      '''\n",
        "      Join the items in the list with a empty string '' : https://www.w3schools.com/python/ref_string_join.asp\n",
        "      If input is [2016, -, 10, -, 31]; then returns 2016-10-31\n",
        "      check: https://stackoverflow.com/questions/13174468/how-do-you-join-all-items-in-a-list/13175535\n",
        "      '''\n",
        "      new_img_name = ''.join(i for i in img_name if i.isdigit() or i == '-')\n",
        "      week_img = str(getEpiWeek(new_img_name))\n",
        "\n",
        "      if week_df == week_img:\n",
        "        img_path = os.path.join(img_folder, img_name)\n",
        "        img = readImg(img_path, resize_ratio)\n",
        "\n",
        "        info_dict['Image'].append(img)\n",
        "        info_dict['LastDayWeek'].append(name)\n",
        "        info_dict['cases_medellin'].append(case)\n",
        "        break\n",
        "\n",
        "  return info_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHiQ5bbm5K7D"
      },
      "source": [
        "# Match Images and Cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nucXPOKEo_b"
      },
      "source": [
        "# Load data from one of the source\n",
        "def loadData(csv_folder, img_folder, option=None, resize_ratio=None):\n",
        "  if option is None:\n",
        "    # Get data by combining from csv and images\n",
        "    df = loadStructuredData(csv_folder)\n",
        "    info_dict = combineData(img_folder, df, resize_ratio)\n",
        "    print(len(info_dict['LastDayWeek']), len(info_dict['Image']), len(info_dict['cases_medellin']))\n",
        "\n",
        "    # Save dictionary into pickle\n",
        "    pickle_folder = '/content/drive/MyDrive/Dengue forecasting with Satellite Images/pickle'\n",
        "    pickle_name = '12_band.pkl'\n",
        "    path = os.path.join(pickle_folder, pickle_name)\n",
        "    saveAsPickle(path, info_dict)\n",
        "\n",
        "  else:\n",
        "    # Load data from previous pickle file\n",
        "    info_dict = loadDataFromPickle(option)\n",
        "  return info_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BVw4_r5sIbDy",
        "outputId": "7064fa5c-a715-4fcf-f0e1-3f86b74ae8b0"
      },
      "source": [
        "# Define main folder\n",
        "dataset_folder = '/content/drive/MyDrive/Dengue forecasting with Satellite Images/dataset'\n",
        "\n",
        "# Define image folder\n",
        "image_name = 'image/image_12_band'\n",
        "img_folder = os.path.join(dataset_folder, image_name)\n",
        "\n",
        "# Define csv folder\n",
        "csv_name = 'csv/merge_cases_timeseries.csv'\n",
        "csv_folder = os.path.join(dataset_folder, csv_name)\n",
        "\n",
        "pickle_folder = '/content/drive/MyDrive/Dengue forecasting with Satellite Images/pickle'\n",
        "pickle_name = '12_band.pkl'\n",
        "path = os.path.join(pickle_folder, pickle_name)\n",
        "info_dict = loadData(csv_folder, img_folder, None, 0.5)\n",
        "\n",
        "print('INFO_DICT'.center(50, '-'))\n",
        "print('keys:', info_dict.keys())\n",
        "print('')\n",
        "\n",
        "print('DENGUE CASES'.center(50, '-'))\n",
        "print('Max weekly dengue cases:', max(info_dict['cases_medellin']))\n",
        "print('Min weekly dengue cases:', min(info_dict['cases_medellin']))\n",
        "print('')\n",
        "\n",
        "print('WEEKS'.center(50, '-'))\n",
        "print('Max week:', max(info_dict['LastDayWeek']))\n",
        "print('Min week:', min(info_dict['LastDayWeek']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2007-01-06\n",
            "2007-01-13\n",
            "2007-01-20\n",
            "2007-01-27\n",
            "2007-02-03\n",
            "2007-02-10\n",
            "2007-02-17\n",
            "2007-02-24\n",
            "2007-03-03\n",
            "2007-03-10\n",
            "2007-03-17\n",
            "2007-03-24\n",
            "2007-03-31\n",
            "2007-04-07\n",
            "2007-04-14\n",
            "2007-04-21\n",
            "2007-04-28\n",
            "2007-05-05\n",
            "2007-05-12\n",
            "2007-05-19\n",
            "2007-05-26\n",
            "2007-06-02\n",
            "2007-06-09\n",
            "2007-06-16\n",
            "2007-06-23\n",
            "2007-06-30\n",
            "2007-07-07\n",
            "2007-07-14\n",
            "2007-07-21\n",
            "2007-07-28\n",
            "2007-08-04\n",
            "2007-08-11\n",
            "2007-08-18\n",
            "2007-08-25\n",
            "2007-09-01\n",
            "2007-09-08\n",
            "2007-09-15\n",
            "2007-09-22\n",
            "2007-09-29\n",
            "2007-10-06\n",
            "2007-10-13\n",
            "2007-10-20\n",
            "2007-10-27\n",
            "2007-11-03\n",
            "2007-11-10\n",
            "2007-11-17\n",
            "2007-11-24\n",
            "2007-12-01\n",
            "2007-12-08\n",
            "2007-12-15\n",
            "2007-12-22\n",
            "2007-12-29\n",
            "2008-01-05\n",
            "2008-01-12\n",
            "2008-01-19\n",
            "2008-01-26\n",
            "2008-02-02\n",
            "2008-02-09\n",
            "2008-02-16\n",
            "2008-02-23\n",
            "2008-03-01\n",
            "2008-03-08\n",
            "2008-03-15\n",
            "2008-03-22\n",
            "2008-03-29\n",
            "2008-04-05\n",
            "2008-04-12\n",
            "2008-04-19\n",
            "2008-04-26\n",
            "2008-05-03\n",
            "2008-05-10\n",
            "2008-05-17\n",
            "2008-05-24\n",
            "2008-05-31\n",
            "2008-06-07\n",
            "2008-06-14\n",
            "2008-06-21\n",
            "2008-06-28\n",
            "2008-07-05\n",
            "2008-07-12\n",
            "2008-07-19\n",
            "2008-07-26\n",
            "2008-08-02\n",
            "2008-08-09\n",
            "2008-08-16\n",
            "2008-08-23\n",
            "2008-08-30\n",
            "2008-09-06\n",
            "2008-09-13\n",
            "2008-09-20\n",
            "2008-09-27\n",
            "2008-10-04\n",
            "2008-10-11\n",
            "2008-10-18\n",
            "2008-10-25\n",
            "2008-11-01\n",
            "2008-11-08\n",
            "2008-11-15\n",
            "2008-11-22\n",
            "2008-11-29\n",
            "2008-12-06\n",
            "2008-12-13\n",
            "2008-12-20\n",
            "2008-12-27\n",
            "2009-01-03\n",
            "2009-01-10\n",
            "2009-01-17\n",
            "2009-01-24\n",
            "2009-01-31\n",
            "2009-02-07\n",
            "2009-02-14\n",
            "2009-02-21\n",
            "2009-02-28\n",
            "2009-03-07\n",
            "2009-03-14\n",
            "2009-03-21\n",
            "2009-03-28\n",
            "2009-04-04\n",
            "2009-04-11\n",
            "2009-04-18\n",
            "2009-04-25\n",
            "2009-05-02\n",
            "2009-05-09\n",
            "2009-05-16\n",
            "2009-05-23\n",
            "2009-05-30\n",
            "2009-06-06\n",
            "2009-06-13\n",
            "2009-06-20\n",
            "2009-06-27\n",
            "2009-07-04\n",
            "2009-07-11\n",
            "2009-07-18\n",
            "2009-07-25\n",
            "2009-08-01\n",
            "2009-08-08\n",
            "2009-08-15\n",
            "2009-08-22\n",
            "2009-08-29\n",
            "2009-09-05\n",
            "2009-09-12\n",
            "2009-09-19\n",
            "2009-09-26\n",
            "2009-10-03\n",
            "2009-10-10\n",
            "2009-10-17\n",
            "2009-10-24\n",
            "2009-10-31\n",
            "2009-11-07\n",
            "2009-11-14\n",
            "2009-11-21\n",
            "2009-11-28\n",
            "2009-12-05\n",
            "2009-12-12\n",
            "2009-12-19\n",
            "2009-12-26\n",
            "2010-01-02\n",
            "2010-01-09\n",
            "2010-01-16\n",
            "2010-01-23\n",
            "2010-01-30\n",
            "2010-02-06\n",
            "2010-02-13\n",
            "2010-02-20\n",
            "2010-02-27\n",
            "2010-03-06\n",
            "2010-03-13\n",
            "2010-03-20\n",
            "2010-03-27\n",
            "2010-04-03\n",
            "2010-04-10\n",
            "2010-04-17\n",
            "2010-04-24\n",
            "2010-05-01\n",
            "2010-05-08\n",
            "2010-05-15\n",
            "2010-05-22\n",
            "2010-05-29\n",
            "2010-06-05\n",
            "2010-06-12\n",
            "2010-06-19\n",
            "2010-06-26\n",
            "2010-07-03\n",
            "2010-07-10\n",
            "2010-07-17\n",
            "2010-07-24\n",
            "2010-07-31\n",
            "2010-08-07\n",
            "2010-08-14\n",
            "2010-08-21\n",
            "2010-08-28\n",
            "2010-09-04\n",
            "2010-09-11\n",
            "2010-09-18\n",
            "2010-09-25\n",
            "2010-10-02\n",
            "2010-10-09\n",
            "2010-10-16\n",
            "2010-10-23\n",
            "2010-10-30\n",
            "2010-11-06\n",
            "2010-11-13\n",
            "2010-11-20\n",
            "2010-11-27\n",
            "2010-12-04\n",
            "2010-12-11\n",
            "2010-12-18\n",
            "2010-12-25\n",
            "2011-01-01\n",
            "2011-01-08\n",
            "2011-01-15\n",
            "2011-01-22\n",
            "2011-01-29\n",
            "2011-02-05\n",
            "2011-02-12\n",
            "2011-02-19\n",
            "2011-02-26\n",
            "2011-03-05\n",
            "2011-03-12\n",
            "2011-03-19\n",
            "2011-03-26\n",
            "2011-04-02\n",
            "2011-04-09\n",
            "2011-04-16\n",
            "2011-04-23\n",
            "2011-04-30\n",
            "2011-05-07\n",
            "2011-05-14\n",
            "2011-05-21\n",
            "2011-05-28\n",
            "2011-06-04\n",
            "2011-06-11\n",
            "2011-06-18\n",
            "2011-06-25\n",
            "2011-07-02\n",
            "2011-07-09\n",
            "2011-07-16\n",
            "2011-07-23\n",
            "2011-07-30\n",
            "2011-08-06\n",
            "2011-08-13\n",
            "2011-08-20\n",
            "2011-08-27\n",
            "2011-09-03\n",
            "2011-09-10\n",
            "2011-09-17\n",
            "2011-09-24\n",
            "2011-10-01\n",
            "2011-10-08\n",
            "2011-10-15\n",
            "2011-10-22\n",
            "2011-10-29\n",
            "2011-11-05\n",
            "2011-11-12\n",
            "2011-11-19\n",
            "2011-11-26\n",
            "2011-12-03\n",
            "2011-12-10\n",
            "2011-12-17\n",
            "2011-12-24\n",
            "2012-01-07\n",
            "2012-01-07\n",
            "2012-01-14\n",
            "2012-01-21\n",
            "2012-01-28\n",
            "2012-02-04\n",
            "2012-02-11\n",
            "2012-02-18\n",
            "2012-02-25\n",
            "2012-03-03\n",
            "2012-03-10\n",
            "2012-03-17\n",
            "2012-03-24\n",
            "2012-03-31\n",
            "2012-04-07\n",
            "2012-04-14\n",
            "2012-04-21\n",
            "2012-04-28\n",
            "2012-05-05\n",
            "2012-05-12\n",
            "2012-05-19\n",
            "2012-05-26\n",
            "2012-06-02\n",
            "2012-06-09\n",
            "2012-06-16\n",
            "2012-06-23\n",
            "2012-06-30\n",
            "2012-07-07\n",
            "2012-07-14\n",
            "2012-07-21\n",
            "2012-07-28\n",
            "2012-08-04\n",
            "2012-08-11\n",
            "2012-08-18\n",
            "2012-08-25\n",
            "2012-09-01\n",
            "2012-09-08\n",
            "2012-09-15\n",
            "2012-09-22\n",
            "2012-09-29\n",
            "2012-10-06\n",
            "2012-10-13\n",
            "2012-10-20\n",
            "2012-10-27\n",
            "2012-11-03\n",
            "2012-11-10\n",
            "2012-11-17\n",
            "2012-11-24\n",
            "2012-12-01\n",
            "2012-12-08\n",
            "2012-12-15\n",
            "2012-12-22\n",
            "2013-01-05\n",
            "2013-01-12\n",
            "2013-01-19\n",
            "2013-01-26\n",
            "2013-02-02\n",
            "2013-02-09\n",
            "2013-02-16\n",
            "2013-02-23\n",
            "2013-03-02\n",
            "2013-03-09\n",
            "2013-03-16\n",
            "2013-03-23\n",
            "2013-03-30\n",
            "2013-04-06\n",
            "2013-04-13\n",
            "2013-04-20\n",
            "2013-04-27\n",
            "2013-05-04\n",
            "2013-05-11\n",
            "2013-05-18\n",
            "2013-05-25\n",
            "2013-06-01\n",
            "2013-06-08\n",
            "2013-06-15\n",
            "2013-06-22\n",
            "2013-06-29\n",
            "2013-07-06\n",
            "2013-07-13\n",
            "2013-07-20\n",
            "2013-07-27\n",
            "2013-08-03\n",
            "2013-08-10\n",
            "2013-08-17\n",
            "2013-08-24\n",
            "2013-08-31\n",
            "2013-09-07\n",
            "2013-09-14\n",
            "2013-09-21\n",
            "2013-09-28\n",
            "2013-10-05\n",
            "2013-10-12\n",
            "2013-10-19\n",
            "2013-10-26\n",
            "2013-11-02\n",
            "2013-11-09\n",
            "2013-11-16\n",
            "2013-11-23\n",
            "2013-11-30\n",
            "2013-12-07\n",
            "2013-12-14\n",
            "2013-12-21\n",
            "2013-12-28\n",
            "2014-01-04\n",
            "2014-01-11\n",
            "2014-01-18\n",
            "2014-01-25\n",
            "2014-02-01\n",
            "2014-02-08\n",
            "2014-02-15\n",
            "2014-02-22\n",
            "2014-03-01\n",
            "2014-03-08\n",
            "2014-03-15\n",
            "2014-03-22\n",
            "2014-03-29\n",
            "2014-04-05\n",
            "2014-04-12\n",
            "2014-04-19\n",
            "2014-04-26\n",
            "2014-05-03\n",
            "2014-05-10\n",
            "2014-05-17\n",
            "2014-05-24\n",
            "2014-05-31\n",
            "2014-06-07\n",
            "2014-06-14\n",
            "2014-06-21\n",
            "2014-06-28\n",
            "2014-07-05\n",
            "2014-07-12\n",
            "2014-07-19\n",
            "2014-07-26\n",
            "2014-08-02\n",
            "2014-08-09\n",
            "2014-08-16\n",
            "2014-08-23\n",
            "2014-08-30\n",
            "2014-09-06\n",
            "2014-09-13\n",
            "2014-09-20\n",
            "2014-09-27\n",
            "2014-10-04\n",
            "2014-10-11\n",
            "2014-10-18\n",
            "2014-10-25\n",
            "2014-11-01\n",
            "2014-11-08\n",
            "2014-11-15\n",
            "2014-11-22\n",
            "2014-11-29\n",
            "2014-12-06\n",
            "2014-12-13\n",
            "2014-12-20\n",
            "2014-12-27\n",
            "2015-01-03\n",
            "2015-01-10\n",
            "2015-01-17\n",
            "2015-01-24\n",
            "2015-01-31\n",
            "2015-02-07\n",
            "2015-02-14\n",
            "2015-02-21\n",
            "2015-02-28\n",
            "2015-03-07\n",
            "2015-03-14\n",
            "2015-03-21\n",
            "2015-03-28\n",
            "2015-04-04\n",
            "2015-04-11\n",
            "2015-04-18\n",
            "2015-04-25\n",
            "2015-05-02\n",
            "2015-05-09\n",
            "2015-05-16\n",
            "2015-05-23\n",
            "2015-05-30\n",
            "2015-06-06\n",
            "2015-06-13\n",
            "2015-06-20\n",
            "2015-06-27\n",
            "2015-07-04\n",
            "2015-07-11\n",
            "2015-07-18\n",
            "2015-07-25\n",
            "2015-08-01\n",
            "2015-08-08\n",
            "2015-08-15\n",
            "2015-08-22\n",
            "2015-08-29\n",
            "2015-09-05\n",
            "2015-09-12\n",
            "2015-09-19\n",
            "2015-09-26\n",
            "2015-10-03\n",
            "2015-10-10\n",
            "2015-10-17\n",
            "2015-10-24\n",
            "2015-10-31\n",
            "2015-11-07\n",
            "image_2015-11-01.tiff\n",
            "    origin shape: (1205, 765, 12)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-8506ec54f461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpickle_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'12_band.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0minfo_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'INFO_DICT'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-c41f798f1c92>\u001b[0m in \u001b[0;36mloadData\u001b[0;34m(csv_folder, img_folder, option, resize_ratio)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Get data by combining from csv and images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadStructuredData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0minfo_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombineData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LastDayWeek'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cases_medellin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-1d55de6337a3>\u001b[0m in \u001b[0;36mcombineData\u001b[0;34m(img_folder, df, resize_ratio)\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mweek_df\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mweek_img\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadImg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0minfo_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-82c637786f50>\u001b[0m in \u001b[0;36mreadImg\u001b[0;34m(img_path, resize_ratio)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresize_ratio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnew_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mresize_ratio\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    resized shape:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \"\"\"\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0moutput_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQTM0ZTP-fNQ"
      },
      "source": [
        "# Data Distrubtion Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdXTQWwKWDQ4"
      },
      "source": [
        "x_axis = info_dict['LastDayWeek']\n",
        "y_axis = info_dict['cases_medellin']\n",
        "plotData(x_axis, y_axis, 'Images & Cases Intersection')\n",
        "\n",
        "dataset_folder = '/content/drive/MyDrive/Dengue forecasting with Satellite Images/dataset'\n",
        "csv_name = 'csv/merge_cases_timeseries.csv'\n",
        "csv_folder = os.path.join(dataset_folder, csv_name)\n",
        "df = loadStructuredData(csv_folder)\n",
        "\n",
        "x_axis = df['LastDayWeek']\n",
        "y_axis = df['cases_medellin'].astype(int)\n",
        "plotData(x_axis, y_axis, 'Cases')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5b2diYM9Awm"
      },
      "source": [
        "# Split Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcq2BFi38MW5",
        "outputId": "179f7d43-4467-4f73-8581-115e63632d21"
      },
      "source": [
        "# Split the data into training (0.8) and testing (0.2)\n",
        "train_val_ratio = 0.8\n",
        "train_num = int(len(info_dict['Image']) * train_val_ratio)\n",
        "\n",
        "# Change list to array\n",
        "origin_dimension_X = np.array(info_dict['Image'])\n",
        "labels = np.array(info_dict['cases_medellin'])\n",
        "\n",
        "# Reshape image from (1205, 765, 12) to (11061900, 1)\n",
        "X = origin_dimension_X.reshape(len(info_dict['Image']), -1)\n",
        "print('Image reshaped from', origin_dimension_X[0].shape, 'to', X[0].shape)\n",
        "print(''.center(60,'-'))\n",
        "\n",
        "origin_X_train = X[:train_num]\n",
        "y_train = labels[:train_num]\n",
        "origin_X_test = X[train_num:]\n",
        "y_test = labels[train_num:]\n",
        "\n",
        "print('Total number of weeks:'.ljust(30), len(X), 'weeks')\n",
        "print('Training input:'.ljust(30), origin_X_train.shape)\n",
        "print('Training output:'.ljust(30), y_train.shape)\n",
        "print('Testing input:'.ljust(30), origin_X_test.shape)\n",
        "print('Testing output:'.ljust(30), y_test.shape) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image reshaped from (1205, 765, 12) to (11061900,)\n",
            "------------------------------------------------------------\n",
            "Total number of weeks:         112 weeks\n",
            "Training input:                (89, 11061900)\n",
            "Training output:               (89,)\n",
            "Testing input:                 (23, 11061900)\n",
            "Testing output:                (23,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m11RwsPq4M89"
      },
      "source": [
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiHKO3QgaJV5"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFu2p9ERQohZ"
      },
      "source": [
        "def calCorrelation(data, label):\n",
        "  corr = np.corrcoef(data, label)[0, 1]\n",
        "  return round(corr, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJg4PwES7L1T"
      },
      "source": [
        "def dimension_reduct_with_PCA(origin_X_train, origin_X_test, y_train, load_from_pickle=False, pickle_name=None):\n",
        "  origin_X_train = np.float32(origin_X_train)\n",
        "  origin_X_test = np.float32(origin_X_test)\n",
        "  y_train = np.float32(y_train)\n",
        "\n",
        "  if load_from_pickle:\n",
        "    # Load PCA model\n",
        "    path = os.path.join(pickle_folder, pickle_name)\n",
        "    pca = loadPickle(path)\n",
        "    pca_X_train = pca.transform(origin_X_train)\n",
        "\n",
        "  else:\n",
        "    # Create PCA object\n",
        "    # Select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components\n",
        "    print('train start')\n",
        "    pca = PCA(n_components=0.95) \n",
        "    pca_X_train = pca.fit_transform(origin_X_train)\n",
        "    print('train end')\n",
        "\n",
        "    # Save PCA model\n",
        "    if pickle_name:\n",
        "      path = os.path.join(pickle_folder, pickle_name)\n",
        "      saveAsPickle(path, pca)\n",
        "\n",
        "  pca_X_test = pca.transform(origin_X_test)\n",
        "  print(origin_X_train.shape)\n",
        "  print(pca_X_train.shape)  \n",
        "\n",
        "  return pca, pca_X_train, pca_X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6IdJ8O33K7s"
      },
      "source": [
        "def plotDistribution(corr_list, X_train, y_train):\n",
        "  sorted_corr_list = list(corr_list)\n",
        "  sorted_corr_list.sort(reverse=True)\n",
        "\n",
        "  max_index = corr_list.index(sorted_corr_list[0])\n",
        "  sec_max_index = corr_list.index(sorted_corr_list[1])\n",
        "\n",
        "  color_list = []\n",
        "  max_cases = max(y_train)\n",
        "\n",
        "  for i in y_train:\n",
        "    color = [i / max_cases, 0, 0]\n",
        "    color_list.append(color)\n",
        "\n",
        "  color_list = np.array(color_list)\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.scatter(X_train[:,max_index], X_train[:,sec_max_index], c=color_list)\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.scatter(np.full(y_train.shape, 0, dtype=int), y_train, c=color_list)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX2O1i8ba7Y2"
      },
      "source": [
        "## LLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e77g9RiC6EHn"
      },
      "source": [
        "def dimension_reduct_with_LLE(origin_X_train, origin_X_test, y_train):\n",
        "  embedding = LocallyLinearEmbedding(n_components=2, neighbors_algorithm='kd_tree')\n",
        "  lle_X_train = embedding.fit_transform(origin_X_train, y_train)\n",
        "  lle_X_test = embedding.transform(origin_X_test)\n",
        "  return lle_X_train, lle_X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2Zl1UhUeHxJ"
      },
      "source": [
        "# Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr76lZu5d-ZB"
      },
      "source": [
        "# Mean Absolute Percentage Error\n",
        "def get_MAPE_score(y_true, y_pred):\n",
        "    # The lower the MAPE the better\n",
        "    # TODO: Consult Dr. Septarshi about zero case (y_true) \n",
        "\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    print(np.mean(np.abs((y_true - y_pred) / y_true)) * 100)\n",
        "    print(mean_absolute_percentage_error(y_true, y_pred))\n",
        "\n",
        "    # Example:\n",
        "    # (0.5 - 0.4) / 0.5 = 0.2\n",
        "    # (0.5 - 0.5) / 0.5 = 0.0 <-> y_true = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJb-HT_4dxmh"
      },
      "source": [
        "def get_MAE_score(y_test, y_pred):\n",
        "  return round(mean_absolute_error(y_test, y_pred), 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i789vF8xHWBL"
      },
      "source": [
        "# Statistical Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEF-AWAf5ka7"
      },
      "source": [
        "def classified_with_LR(X_train, X_test, y_train, y_test):\n",
        "  regr = linear_model.LinearRegression()\n",
        "  regr.fit(X_train, y_train)\n",
        "  y_pred = regr.predict(X_test)\n",
        "  MAE = get_MAE_score(y_test, y_pred)\n",
        "  get_MAPE_score((y_test, y_pred))\n",
        "  return MAE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqTl3o547XQ6"
      },
      "source": [
        "def classified_with_SVR(X_train, X_test, y_train, y_test):\n",
        "  regressor = SVR(C=1.0, epsilon=0.2)\n",
        "  regressor.fit(X_train, y_train)\n",
        "  y_pred = regressor.predict(X_test)\n",
        "  MAE = get_MAE_score(y_test, y_pred)\n",
        "  get_MAPE_score(y_test, y_pred)\n",
        "  return MAE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFkbWi2aPqv3"
      },
      "source": [
        "# Test Different Combination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtH4fUKLOojP",
        "outputId": "ab9a943c-3170-4d1a-ae05-407a2e065c92"
      },
      "source": [
        "# PCA\n",
        "pca, pca_X_train, pca_X_test = dimension_reduct_with_PCA(origin_X_train, origin_X_test, y_train)\n",
        "\n",
        "# Linear Regression\n",
        "# MAE = classified_with_LR(pca_X_train, pca_X_test, y_train, y_test)\n",
        "# print('[PCA][LR] MAE', MAE)\n",
        "\n",
        "# SVR \n",
        "# MAE = classified_with_SVR(pca_X_train, pca_X_test, y_train, y_test)\n",
        "# print('[PCA][SVR] MAE', MAE)\n",
        "\n",
        "classified_with_LR(origin_X_train, origin_X_test, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train start\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uobxOq7ufo2u"
      },
      "source": [
        "print(pca.components_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zjvq7Pgysk5"
      },
      "source": [
        "## Principle Component Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--cq8_0JeEwa"
      },
      "source": [
        "fig = plt.figure(figsize=(8,16)) \n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "\n",
        "for i in range(len(pca.components_)): \n",
        "  ax = fig.add_subplot(len(pca.components_)/5 + 1, 5, i+1, xticks=[], yticks=[]) \n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  rgb_pca = pca.components_[i,:].reshape(1205, 765, 3)\n",
        "  norm_pc_img = scaler.fit_transform(rgb_pca[:,:,2].reshape(-1, 1))\n",
        "\n",
        "  ax.imshow(np.reshape(norm_pc_img, (1205, 765)), cmap=plt.cm.seismic, interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1iuFs6_K7Df"
      },
      "source": [
        "# Image1 x PC\n",
        "fig = plt.figure(figsize=(8,16)) \n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "img = origin_X_train[0].reshape(1205, 765, 3)\n",
        "\n",
        "for i in range(len(pca.components_)): \n",
        "  ax = fig.add_subplot(len(pca.components_)/5 + 1, 5, i+1, xticks=[], yticks=[]) \n",
        "  pc_img = np.multiply(pca.components_[i,:].reshape(1205, 765, 3), img)\n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  norm_pc_img = scaler.fit_transform(pc_img.reshape(-1, 1))\n",
        "  ax.imshow(np.reshape(norm_pc_img, (1205, 765, 3)), cmap=plt.cm.bone, interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsyl8ihibVP4"
      },
      "source": [
        "fig = plt.figure(figsize=(8,16)) \n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "\n",
        "# Image shape = (1, 1205 x 765 x 3)\n",
        "# 51 PC = (1, 1205 x 765 x 3)\n",
        "\n",
        "for i in range(len(pca.components_)): \n",
        "  ax = fig.add_subplot(len(pca.components_)/5 + 1, 5, i+1, xticks=[], yticks=[]) \n",
        "\n",
        "  # pca.components_[i,:] = (1205 x 765 x 3, 1) -> (1205, 765, 3)\n",
        "  rgb_pca = pca.components_[i,:].reshape(1205, 765, 3)\n",
        "  \n",
        "  # Normalize values to range 0 ~ 1\n",
        "  scaler = MinMaxScaler(feature_range=(0,1)) \n",
        "\n",
        "  # rgb_pca[:,:,0]: H(row), W(column), RGB Channel\n",
        "  # rgb_pca[:,:,0] = (1205, 765, 1) -> (1205 x 765, 1)\n",
        "  # TODO: Check dimension of (rgb_pca[:,:,0].reshape(-1, 1))\n",
        "  norm_pc_img = scaler.fit_transform(rgb_pca[:,:,0].reshape(-1, 1)) \n",
        "  \n",
        "  # (921825,1) -> (1205, 765)\n",
        "  ax.imshow(np.reshape(norm_pc_img, (1205, 765)), cmap=plt.cm.seismic, interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY7u7AxKH3QY"
      },
      "source": [
        "fig = plt.figure(figsize=(8,16)) \n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "\n",
        "for i in range(len(pca.components_)): \n",
        "  ax = fig.add_subplot(len(pca.components_)/5 + 1, 5, i+1, xticks=[], yticks=[]) \n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  rgb_pca = pca.components_[i,:].reshape(1205, 765, 3)\n",
        "  norm_pc_img = scaler.fit_transform(rgb_pca[:,:,1].reshape(-1, 1))\n",
        "  ax.imshow(np.reshape(norm_pc_img, (1205, 765)), cmap=plt.cm.seismic, interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwMABr4ubYdv"
      },
      "source": [
        "fig = plt.figure(figsize=(8,16)) \n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05) \n",
        "\n",
        "for i in range(len(pca.components_)): \n",
        "  ax = fig.add_subplot(len(pca.components_)/5 + 1, 5, i+1, xticks=[], yticks=[]) \n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  rgb_pca = pca.components_[i,:].reshape(1205, 765, 3)\n",
        "  norm_pc_img = scaler.fit_transform(rgb_pca[:,:,2].reshape(-1, 1))\n",
        "  # TODO: choose pc graph with |corr| > 0.2\n",
        "  # TODO: make a average input image to overlap with PC figure\n",
        "  # TODO: Ask david about the lines inside images\n",
        "  ax.imshow(np.reshape(norm_pc_img, (1205, 765)), cmap=plt.cm.seismic, interpolation='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMJik36OwX_d"
      },
      "source": [
        "lle_X_train, lle_X_test = dimension_reduct_with_LLE(origin_X_train, origin_X_test, y_train)\n",
        "\n",
        "# Linear Regression\n",
        "MAE = classified_with_LR(lle_X_train, lle_X_test, y_train, y_test)\n",
        "print('[LLE][LR] MAE', MAE)\n",
        "\n",
        "# SVR \n",
        "MAE = classified_with_SVR(lle_X_train, lle_X_test, y_train, y_test)\n",
        "print('[LLE][SVR] MAE', MAE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz97nKPvw4pf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYJyry6Foglw"
      },
      "source": [
        "# Save PC to drive\n",
        "img = origin_X_train[0].reshape(1205, 765, 3)\n",
        "plt.imshow(img)\n",
        "img_number_per_row = 3\n",
        "total_pc = len(pca.components_)\n",
        "print(total_pc)\n",
        "\n",
        "for index, pc in enumerate(pca.components_):\n",
        "  # plt.subplot(total_pc / img_number_per_row + 1, \n",
        "  #             img_number_per_row, \n",
        "  #             index + 1)\n",
        "  # pc_img = np.multiply(pc.reshape(1205, 765, 3), img)\n",
        "\n",
        "  pc_img = pc.reshape(1205, 765, 3)\n",
        "  scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  norm_pc_img = scaler.fit_transform(pc_img.reshape(-1, 1)).reshape(1205, 765, 3)\n",
        "  plt.imshow(norm_pc_img)\n",
        "  plt.savefig('/content/drive/MyDrive/Dengue forecasting with Satellite Images/pca_components/' + str(index) + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBYN_hgJBRxb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}